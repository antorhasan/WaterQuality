{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.framework import ops\nimport pandas as pd\n\ndf=pd.read_csv(\"D:\\Thesis\\messy work\\Final.csv\")\ndf['TURBIDITY']=pd.to_numeric(df['TURBIDITY'],errors='coerce')\ndf['Labels']=df.Labels.astype(float)\ndf=df[['TEMPL','PHL','EC','CHLORIDE','TALKAL','TURBIDITY','DO','BOD','Labels']]\n\nXRaw=np.array(df)\nXRaw=np.random.permutation(XRaw)\n\ndef norm(X):\n    meanAr=np.mean(X,axis=1)\n    meanAr=np.reshape(meanAr,(meanAr.shape[0],1))\n    varAr=np.var(X,axis=1)\n    varAr=np.reshape(varAr,(varAr.shape[0],1))\n    normX=np.divide(np.subtract(X,meanAr),varAr)\n    return normX,meanAr,varAr\n\ndef normRet(X,XTrainMean,XTrainVar):\n    normX=np.divide(np.subtract(X,XTrainMean),XTrainVar)\n    return normX"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"Val=XRaw[0:400,:]\nTest=XRaw[400:800,:]\nTrain=XRaw[800:-1,:]\n\ndef oneHot(labels,C):\n    C=tf.constant(C)\n    oneHotMat=tf.one_hot(labels,C,axis=0)\n    sess=tf.Session()\n    oneHot=sess.run(oneHotMat)\n    sess.close()\n    return oneHot\n\nXTrainR=np.transpose(Train[:,0:-1])\nXTrain,Xmean,Xvar=norm(XTrainR)\nYTrain=Train[:,-1]\nYTrain=np.transpose(YTrain)    #No need to reshape as one_hot handles the the missing dimension automatically\nYTrain=oneHot(YTrain,10)\n\nXTestR=np.transpose(Test[:,0:-1])\nXTest=normRet(XTestR,Xmean,Xvar)\nYTest=Test[:,-1]\nYTest=np.transpose(YTest)\nYTest=oneHot(YTest,10)\n\nXValR=np.transpose(Val[:,0:-1])\nXVal=normRet(XValR,Xmean,Xvar)\nYVal=Val[:,-1]\nYVal=np.transpose(YVal)\nYVal=oneHot(YVal,10)\n"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"print(XTrainR,XTrainR.shape)\nprint(YTrain,YTrain.shape)\nprint(XTrain,XTrain.shape)"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"\ndef createPlace(nX,nY):\n    X=tf.placeholder(tf.float32,shape=(nX,None))\n    Y=tf.placeholder(tf.float32,shape=(nY,None))\n    return X,Y\n\ndef initParameters():\n    W1=tf.get_variable(\"W1\",[15,8],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n    b1=tf.get_variable(\"b1\",[15,1],initializer=tf.zeros_initializer(),dtype=tf.float32)\n    W2=tf.get_variable(\"W2\",[15,15],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n    b2=tf.get_variable(\"b2\",[15,1],initializer=tf.zeros_initializer(),dtype=tf.float32)\n    W3=tf.get_variable(\"W3\",[10,15],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n    b3=tf.get_variable(\"b3\",[10,1],initializer=tf.zeros_initializer(),dtype=tf.float32)\n\n    parameters = {\"W1\":W1,\n                  \"b1\":b1,\n                  \"W2\":W2,\n                  \"b2\":b2,\n                  \"W3\":W3,\n                  \"b3\":b3}\n    return parameters\n\ndef forward(X,parameters):\n    W1=parameters['W1']\n    b1=parameters['b1']\n    W2=parameters['W2']\n    b2=parameters['b2']\n    W3=parameters['W3']\n    b3=parameters['b3']\n\n    Z1=tf.add(tf.matmul(W1,X),b1)\n    A1=tf.nn.relu(Z1)\n    Z2=tf.add(tf.matmul(W2,A1),b2)\n    A2=tf.nn.relu(Z2)\n    Z3=tf.add(tf.matmul(W3,A2),b3)\n\n    return Z3\n\ndef compCost(Z3,Y):\n    logits=tf.transpose(Z3)\n    labels=tf.transpose(Y)\n\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n    return cost"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"def model(XTrainR,YTrain,XValR,YVal,learning_rate=.001,num_epochs=1000,print_cost=True):\n    ops.reset_default_graph()\n    (nX,m)=XTrainR.shape\n    nY=YTrain.shape[0]\n    costs=[]\n\n    X,Y=createPlace(nX,nY)\n    parameters=initParameters()\n    Z3=forward(X,parameters)\n    cost=compCost(Z3,Y)\n\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n    init = tf.global_variables_initializer()\n\n    with tf.Session() as sess:\n        sess.run(init)\n        print(XTrainR)\n        print(YTrain)\n        for epoch in range(num_epochs):\n            epoch_cost=0\n            _,epoch_cost=sess.run([optimizer,cost],feed_dict={X:XTrainR,Y:YTrain})\n            #print(epoch_cost)\n            if print_cost==True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch,epoch_cost))\n            if print_cost==True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\"+str(learning_rate))\n        plt.show()\n\n        parameters=sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        correct_prediction=tf.equal(tf.argmax(Z3),tf.argmax(Y))\n\n        accuracy=tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print (\"Train Accuracy:\",accuracy.eval({X:XTrainR,Y:YTrain}))\n        print (\"Test Accuracy:\", accuracy.eval({X:XValR,Y:YVal}))\n\n        return parameters\n\n"},{"cell_type":"code","execution_count":13,"metadata":{"scrolled":false},"outputs":[],"source":"parameters=model(XTrain,YTrain,XVal,YVal,learning_rate=.001,num_epochs=10000,print_cost=True)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}